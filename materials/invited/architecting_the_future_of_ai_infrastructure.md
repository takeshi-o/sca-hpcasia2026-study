# 📚 Architecting the Future of AI Infrastructure
## SCA/HPC Asia 2026 事前勉強資料

---

## 📋 発表情報

| 項目 | 内容 |
|------|------|
| **セッション名** | Invited Session: Architecting the future of AI infrastructure |
| **日時** | 2026年1月27日（火）11:30 - 12:30 |
| **会場** | グランキューブ大阪（大阪国際会議場） |
| **形式** | パネルディスカッション |
| **主催者** | Eric Van Hensbergen (Arm) |
| **関連度** | ⭐⭐⭐⭐⭐（AI/HPC融合時代の基盤技術） |

### パネリスト一覧

| 氏名 | 所属 | 専門分野 |
|------|------|----------|
| Eric Van Hensbergen | Arm | システムリサーチ、エッジコンピューティング |
| Satoshi Matsuoka（松岡 聡） | RIKEN R-CCS | スーパーコンピュータ「富岳」開発 |
| Jay Boisseau | Google HPC | クラウドHPC、AIインフラ |
| Jason Haga | AIST（産総研） | データ集約型研究、可視化 |
| Dan Ernst | NVIDIA | スーパーコンピューティング製品 |
| Jennifer Glore | Rebellions | AI推論チップ製品管理 |

---

## 🎯 この発表を理解するための前提知識

### 1️⃣ AI インフラストラクチャとは何か？

AIインフラストラクチャとは、**AIモデルを学習・推論するために必要なハードウェアとソフトウェアの総称**です。

```
┌─────────────────────────────────────────────────────────────────┐
│                    AI インフラストラクチャの全体像                   │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │                    アプリケーション層                      │   │
│  │    ChatGPT, 画像生成AI, 自動運転, 科学シミュレーション      │   │
│  └─────────────────────────────────────────────────────────┘   │
│                              ↓                                  │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │                    ソフトウェア層                         │   │
│  │    PyTorch, TensorFlow, CUDA, コンパイラ, OS             │   │
│  └─────────────────────────────────────────────────────────┘   │
│                              ↓                                  │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │                    ネットワーク層                         │   │
│  │    NVLink, InfiniBand, Ethernet, 光ファイバー            │   │
│  └─────────────────────────────────────────────────────────┘   │
│                              ↓                                  │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │                    コンピュート層                         │   │
│  │    GPU (NVIDIA), TPU, CPU (Arm/x86), 専用AIチップ        │   │
│  └─────────────────────────────────────────────────────────┘   │
│                              ↓                                  │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │                    ファシリティ層                         │   │
│  │    電力供給, 冷却システム, ラック, データセンター建屋      │   │
│  └─────────────────────────────────────────────────────────┘   │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

**高校生向け解説**：
スマートフォンでChatGPTを使うとき、実際の計算は世界中のデータセンターにある巨大なコンピュータが行っています。このデータセンターの「コンピュータ」は、普通のPCとは全く違う特殊な構成になっています。それがAIインフラストラクチャです。

---

### 2️⃣ GPU（Graphics Processing Unit）の役割

GPUはもともとゲームの3D映像を描画するために作られましたが、**大量の単純な計算を同時に行う能力**がAI学習に最適であることが発見されました。

```
┌────────────────────────────────────────────────────────────┐
│                    CPU vs GPU の違い                        │
├────────────────────────────────────────────────────────────┤
│                                                            │
│  CPU（少数精鋭）           GPU（大量並列）                  │
│  ┌───┐ ┌───┐              ┌─┬─┬─┬─┬─┬─┬─┬─┬─┬─┬─┬─┬─┬─┬─┐ │
│  │ ● │ │ ● │              │●│●│●│●│●│●│●│●│●│●│●│●│●│●│●│ │
│  └───┘ └───┘              ├─┼─┼─┼─┼─┼─┼─┼─┼─┼─┼─┼─┼─┼─┼─┤ │
│  ┌───┐ ┌───┐              │●│●│●│●│●│●│●│●│●│●│●│●│●│●│●│ │
│  │ ● │ │ ● │              ├─┼─┼─┼─┼─┼─┼─┼─┼─┼─┼─┼─┼─┼─┼─┤ │
│  └───┘ └───┘              │●│●│●│●│●│●│●│●│●│●│●│●│●│●│●│ │
│                           └─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─┴─┘ │
│  8-64コア                  数千〜数万コア                   │
│  複雑な処理が得意          単純な計算の並列処理が得意       │
│                                                            │
└────────────────────────────────────────────────────────────┘
```

**現在の主要GPU**：
- **NVIDIA Blackwell (B200/B300)**: 最新世代、AI学習・推論の両方に最適化
- **AMD Instinct MI300X**: NVIDIAの競合、HBMメモリ大容量
- **Intel Gaudi**: データセンター向けAIアクセラレータ

**高校生向け解説**：
CPUは「なんでも器用にこなせる少数のエリート社員」、GPUは「単純作業を超高速でこなす大量のアルバイト」のようなものです。AIの学習は、同じような計算を何兆回も繰り返すので、GPUの方が圧倒的に速いのです。

---

### 3️⃣ HBM（High Bandwidth Memory）- 超高速メモリ

AIモデルは巨大なデータを扱うため、**メモリの速度がボトルネック**になることが多いです。HBMは、従来のメモリを何層にも積み重ねて超高速化した特殊なメモリです。

```
┌────────────────────────────────────────────────────────────┐
│                    HBM の構造                               │
├────────────────────────────────────────────────────────────┤
│                                                            │
│  従来のDDR5メモリ              HBM3Eメモリ                  │
│                                                            │
│  ┌─────────┐                   ┌─────────┐ ← 12層積層      │
│  │ メモリ  │                   ├─────────┤                 │
│  └─────────┘                   ├─────────┤                 │
│      │                         ├─────────┤                 │
│   256本の                      ├─────────┤                 │
│   配線                         ├─────────┤                 │
│      │                         ├─────────┤                 │
│  ┌───────┐                     ├─────────┤                 │
│  │ CPU   │                     ├─────────┤                 │
│  └───────┘                     ├─────────┤                 │
│                                ├─────────┤                 │
│  帯域: ~50GB/s                 ├─────────┤                 │
│                                ├─────────┤                 │
│                                └─────────┘                 │
│                                    │ 1024本以上の配線       │
│                                ┌───────┐                   │
│                                │ GPU   │                   │
│                                └───────┘                   │
│                                帯域: 1TB/s以上             │
│                                                            │
└────────────────────────────────────────────────────────────┘
```

**最新HBM3E仕様**：
- 帯域幅: 最大1.2TB/s（DDR5の約20倍）
- 容量: 1スタックあたり最大36GB
- NVIDIA GB300では288GB/GPU搭載

**高校生向け解説**：
普通のメモリが「1車線の道路」だとすると、HBMは「20車線の高速道路」のようなものです。AIが必要とする膨大なデータを、一度にたくさん運べるので、計算が止まらずに済みます。

---

### 4️⃣ ネットワーク相互接続技術

大規模AIクラスタでは、数千台のGPUを超高速で接続する必要があります。

```
┌────────────────────────────────────────────────────────────┐
│                ネットワーク階層と技術                        │
├────────────────────────────────────────────────────────────┤
│                                                            │
│  【ラック内（サーバ内）】                                    │
│  ┌─────────────────────────────────────────────────┐      │
│  │  GPU ←─NVLink─→ GPU ←─NVLink─→ GPU ←─NVLink─→ GPU │      │
│  │                    1.8 TB/s                      │      │
│  └─────────────────────────────────────────────────┘      │
│                          │                                  │
│  【ラック間】              ↓                                 │
│  ┌──────────┐    InfiniBand     ┌──────────┐              │
│  │ Server 1 │ ←───────────────→ │ Server 2 │              │
│  └──────────┘    800 Gbps        └──────────┘              │
│                                                            │
│  【データセンター間】                                        │
│  ┌──────────┐    Ethernet/光     ┌──────────┐             │
│  │  DC 東京  │ ←───────────────→ │  DC 大阪  │             │
│  └──────────┘    400 Gbps        └──────────┘             │
│                                                            │
└────────────────────────────────────────────────────────────┘
```

**主要な相互接続技術**：

| 技術 | 帯域幅 | 用途 | 特徴 |
|------|--------|------|------|
| **NVLink 5** | 1.8 TB/s | GPU間（同一ノード内） | 最高速、GPUメモリ共有可能 |
| **InfiniBand NDR/XDR** | 400-800 Gbps | ノード間 | 低遅延、ロスレス、HPC標準 |
| **Ultra Ethernet** | 400-800 Gbps | ノード間 | 低コスト、広く普及 |

**高校生向け解説**：
1つのAIモデルを学習するとき、計算を複数のGPUに分担させます。その際、GPUどうしが頻繁にデータをやり取りする必要があります。このやり取りが遅いと、GPUが待ち状態になって無駄になります。超高速ネットワークは、この「待ち時間」をなくすためにあります。

---

### 5️⃣ 冷却技術 - 液体冷却の時代へ

最新のAI用GPUは1枚で1,000W以上の電力を消費し、膨大な熱を発生させます。

```
┌────────────────────────────────────────────────────────────┐
│                    冷却技術の進化                           │
├────────────────────────────────────────────────────────────┤
│                                                            │
│  【空冷】                【液冷（Direct-to-Chip）】         │
│                                                            │
│  ┌─────────┐            ┌─────────┐                       │
│  │ ファン  │            │冷却板   │← 冷却液が直接接触     │
│  │ ↓↓↓↓↓  │            ├─────────┤                       │
│  │ ■■■■■ │←ヒートシンク│  GPU    │                       │
│  │  GPU   │            └─────────┘                       │
│  └─────────┘                │ │                           │
│                          冷水 ↓ ↑ 温水                     │
│  限界: ~500W/chip        ┌───────────┐                    │
│                          │ クーラー  │                    │
│                          └───────────┘                    │
│                          対応: 1,400W/chip以上            │
│                                                            │
│  【浸漬冷却（Immersion）】                                  │
│  ┌─────────────────────────────┐                          │
│  │  ～～～～～～～～～～～～～  │← 絶縁性の液体に         │
│  │  ～ GPU  GPU  GPU  GPU ～  │  サーバごと浸す         │
│  │  ～～～～～～～～～～～～～  │                          │
│  └─────────────────────────────┘                          │
│                                                            │
└────────────────────────────────────────────────────────────┘
```

**2025年の冷却トレンド**：
- **液体冷却が必須に**: NVIDIA B200は1,200W、B300は1,400W TDPで空冷では対応不可
- **PUE改善**: 液体冷却で電力効率が最大45%向上
- **熱密度**: AIラックは50kW/ラック以上（従来の10倍以上）

**高校生向け解説**：
家庭用ゲームPCが300W程度なのに対し、最新AIチップは1枚で1,400W。これを何百枚も並べると、小さな発電所くらいの熱が出ます。空気だけでは冷やせないので、水（正確には専用の冷却液）を使って直接チップを冷やす技術が必須になっています。

---

### 6️⃣ Arm アーキテクチャのデータセンター進出

スマートフォンで使われてきたArm CPUが、データセンターでも急速に普及しています。

```
┌────────────────────────────────────────────────────────────┐
│                 Arm vs x86 in データセンター                │
├────────────────────────────────────────────────────────────┤
│                                                            │
│  従来のx86サーバ              Arm Neoverse サーバ          │
│  (Intel/AMD)                                               │
│                                                            │
│  ┌─────────────┐              ┌─────────────┐             │
│  │  高性能     │              │  高効率     │             │
│  │  高消費電力 │              │  低消費電力 │             │
│  │             │              │             │             │
│  │  TDP: 350W  │              │  TDP: 250W  │             │
│  └─────────────┘              └─────────────┘             │
│                                                            │
│  2025年現在:                                               │
│  • AWS Graviton (Arm) が主流に                             │
│  • Google Axion、Azure Cobalt も Arm ベース                │
│  • Neoverse が大手ハイパースケーラーの50%シェア目標         │
│                                                            │
└────────────────────────────────────────────────────────────┘
```

**Arm Neoverse の特徴**：
- **高い電力効率**: 性能あたりの消費電力が低い
- **NVLink Fusion対応**: NVIDIAのGPUと高速接続可能
- **主要採用例**: AWS Graviton, Google Axion, Azure Cobalt, NVIDIA Grace

**高校生向け解説**：
皆さんのスマホに入っているチップはほぼ全てArmベースです。「電池を長持ちさせながら高性能」という設計思想が、電気代が巨大なデータセンターでも重要視されるようになりました。

---

## 📖 発表概要

### 🎯 パネルの目的

AIワークロードからの**計算需要が指数関数的に成長**する中、インフラストラクチャ要件はますます困難になっています。このパネルでは：

1. **AIアクセラレータ**がどう進化しているか
2. **計算ラック**の設計がどう変わっているか
3. **AIクラスタ**全体がどう構成されるべきか

を、業界リーダーが議論します。

### 📊 期待される発表内容

パネリストは以下の観点から将来のイノベーションを議論：

```
┌────────────────────────────────────────────────────────────┐
│              議論が期待されるトピック                        │
├────────────────────────────────────────────────────────────┤
│                                                            │
│  1. コンピュート         2. アクセラレータ                  │
│  ┌──────────────┐       ┌──────────────┐                  │
│  │ CPU設計の進化 │       │ GPU/TPU/ASIC │                  │
│  │ Arm vs x86   │       │ の進化と選択 │                  │
│  └──────────────┘       └──────────────┘                  │
│                                                            │
│  3. ネットワーキング     4. 光学技術                        │
│  ┌──────────────┐       ┌──────────────┐                  │
│  │ IB vs Ethernet│       │ Co-packaged  │                  │
│  │ 800G → 1.6T  │       │ Optics      │                  │
│  └──────────────┘       └──────────────┘                  │
│                                                            │
│  5. 冷却技術                                               │
│  ┌──────────────┐                                         │
│  │ 液体冷却必須化│                                         │
│  │ 熱回収・再利用│                                         │
│  └──────────────┘                                         │
│                                                            │
└────────────────────────────────────────────────────────────┘
```

---

## 🔬 技術的詳細

### 🧪 現在のAIインフラの課題

| 課題 | 詳細 | 影響 |
|------|------|------|
| **電力制約** | 1ラック100kW超、データセンター全体でGW級 | 電力網の限界 |
| **熱密度** | 50kW/ラック以上 | 空冷では対応不可 |
| **ネットワークボトルネック** | GPU間通信が学習速度を制限 | 効率低下 |
| **メモリ帯域** | モデルサイズ増大でHBMも限界に | スケーリング制約 |
| **コスト** | GPU 1台 $30,000以上 | ROI確保が困難 |

### 📈 最新技術動向（2025年）

**NVIDIA GB300 NVL72（Blackwell Ultra）**：
- 72 GPU + 36 Grace CPU を1ラックに統合
- 1.1 エクサFLOPS（FP4）の処理能力
- 消費電力: 約120kW/ラック
- 全GPU間がNVLinkで接続（130TB/s）

**ネットワーキングの進化**：
- Ultra Ethernet Consortium (UEC) 1.0仕様リリース
- InfiniBandとEthernetの「二刀流」が主流に
- 800Gbps → 1.6Tbps への移行

**冷却の転換点**：
- 液体冷却がデフォルトに
- TSMC: 2027年にDirect-to-Silicon冷却を商用化予定

### 🔍 パネリスト各社の視点

| 組織 | 主要な視点 |
|------|------------|
| **Arm** | 省電力CPU、Neoverseエコシステム、NVLink Fusion |
| **RIKEN** | 「富岳」運用経験、次世代機「富岳NEXT」構想 |
| **Google HPC** | クラウドHPC、ハイブリッドアプローチ |
| **AIST** | 国内産業応用、可視化技術 |
| **NVIDIA** | Blackwell世代、NVLink/InfiniBand |
| **Rebellions** | AI推論特化チップ、韓国発スタートアップ |

---

## 👤 発表者プロフィール

### Eric Van Hensbergen（主催者）

| 項目 | 内容 |
|------|------|
| **現職** | Arm Research Fellow、システムリサーチグループリーダー |
| **所在地** | テキサス州オースティン |
| **専門分野** | 分散システム、エッジコンピューティング、HPC、OS |
| **前職** | IBM Austin Research Lab（12年間）、ベル研究所（4年間） |
| **研究テーマ** | 数十万〜数百万コアの分散システム、エネルギー効率の良いHPC |

**主な業績**：
- Plan 9、Infernoオペレーティングシステムの開発
- Blue Genesスーパーコンピュータの分散OS開発
- 100本以上の学術論文発表

---

### Satoshi Matsuoka（松岡 聡）

| 項目 | 内容 |
|------|------|
| **現職** | 理化学研究所 計算科学研究センター（R-CCS）センター長 |
| **学歴** | 東京大学 博士（1993年） |
| **専門分野** | ハイパフォーマンスコンピューティング、並列分散システム |

**受賞歴**（抜粋）：
- ACM Gordon Bell Prize（2011年、2021年）
- IEEE Sidney Fernbach Award（2014年）
- IEEE Seymour Cray Award（2022年）- 両賞を受賞した唯一の人物
- 紫綬褒章（2022年）
- HPCwire 35 Legends（2024年）

**主なプロジェクト**：
- **TSUBAME**: 東工大スーパーコンピュータ開発
- **富岳（Fugaku）**: 世界初の4冠スパコン（Top500, HPCG, HPL-AI, Graph500）
- **富岳NEXT**: 2029年稼働予定のゼタスケール計算機

---

### Jay Boisseau

| 項目 | 内容 |
|------|------|
| **現職** | Google Cloud HPC チーム、先進コンピューティングストラテジスト |
| **学歴** | バージニア大学（天文学・物理学）、テキサス大学オースティン校（天文学修士） |
| **経験** | 25年以上の先進コンピューティング分野でのリーダーシップ |

**キャリア**：
- Texas Advanced Computing Center (TACC) 創設者・初代ディレクター
- サンディエゴスーパーコンピュータセンター 科学計算部門アソシエイトディレクター
- Dell Technologies シニアHPCストラテジスト（8年間）

---

### Jason Haga

| 項目 | 内容 |
|------|------|
| **現職** | 産業技術総合研究所（AIST）主席研究員、デジタルアーキテクチャ推進センター企画マネージャー |
| **専門分野** | データ集約型研究、没入型可視化、バイオインフォマティクス |
| **国際活動** | Research Data Alliance (RDA) 評議員、PRAGMA運営委員 |

**主な研究分野**：
- 自然災害管理のための没入型可視化
- 大規模バーチャルスクリーニングのためのバイオインフォマティクスワークフロー
- 連合学習とAI

---

### Dan Ernst

| 項目 | 内容 |
|------|------|
| **現職** | NVIDIA スーパーコンピューティング製品シニアディレクター |
| **学歴** | ミシガン大学 博士（コンピュータサイエンス・エンジニアリング） |
| **研究分野** | 高性能・低電力・フォールトトレラントマイクロアーキテクチャ |

**前職**：
- Microsoft Azure メモリアーキテクチャチームリーダー
- Cray エクサスケールコンピューティングプログラム主任研究員・アーキテクト

---

### Jennifer Glore

| 項目 | 内容 |
|------|------|
| **現職** | Rebellions 製品管理担当エグゼクティブバイスプレジデント |
| **役割** | 開発チームとエンドユーザーの調整、顧客中心製品の提供 |
| **拠点** | パロアルト、カリフォルニア州 |

**Rebellionsについて**：
- 2020年創業、韓国・城南市に本社
- アジア最速成長のAI推論チップ企業
- 主力製品: REBEL-Quad（チップレットアーキテクチャ、HBM3Eメモリ搭載）
- 2025年シリコンバレーVCから資金調達完了、米国子会社設立

---

## 🔗 関連研究

### 📄 主要論文・リソース

| タイトル | 著者/組織 | 年 | 概要 |
|----------|-----------|-----|------|
| Hardware Accelerators for AI | Springer | 2024 | AIアクセラレータの包括的解説 |
| The Memory Wall Problem | Semi Analysis | 2025 | HBMの技術ロードマップ |
| Networking Blueprints for AI | Arrcus/AvidThink | 2025 | AI時代のネットワーク設計 |
| Infinite Scale: Azure AI Superfactory | Microsoft | 2025 | Azureの大規模AIインフラ構築 |

### 🛠️ 関連プラットフォーム・製品

| 製品/プラットフォーム | 企業 | カテゴリ | 特徴 |
|----------------------|------|----------|------|
| GB300 NVL72 | NVIDIA | AIシステム | 72 GPU統合、120kW/ラック |
| Neoverse CSS V3 | Arm | CPUプラットフォーム | NVLink Fusion対応 |
| REBEL-Quad | Rebellions | 推論チップ | HBM3E、チップレット設計 |
| Fugaku | 富士通/理研 | スーパーコンピュータ | Arm A64FX、4冠達成 |
| Spectrum-X | NVIDIA | Ethernetファブリック | AI最適化Ethernet |

---

## 💭 聴講のポイント

### ❓ 質問候補

| 質問 | 意図 |
|------|------|
| 「2030年のAIデータセンターで、空冷が生き残る余地はあるか？」 | 冷却技術の将来方向性を確認 |
| 「InfiniBandとEthernetの住み分けは今後どうなるか？」 | ネットワーク選択の判断基準を理解 |
| 「Armがx86に対して持つ真の優位性は何か？」 | CPUアーキテクチャ選択の根拠を把握 |
| 「専用AIチップ（ASIC）とGPUの競合はどう進展するか？」 | アクセラレータ市場の将来を予測 |
| 「電力制約がAI発展のボトルネックになる時期は？」 | インフラ限界の時間軸を理解 |
| 「富岳NEXTでどのような技術革新を計画しているか？」 | 日本のHPC戦略を把握 |

### 🏭 マツダAI駆動開発への示唆

| 示唆 | 詳細 | 適用場面 |
|------|------|----------|
| **クラウド vs オンプレ判断** | 大規模学習はクラウド、推論・機密データ処理はオンプレの使い分け | 自動運転AIモデル開発 |
| **電力効率の重視** | Armベース・省電力設計は工場エッジAIに有効 | 生産ライン品質検査AI |
| **冷却設計の考慮** | 将来のAIサーバ導入時は液冷前提で設計 | 社内データセンター更新計画 |
| **ネットワーク選択** | 中規模クラスタならEthernet、大規模ならIB検討 | CAEシミュレーション環境 |
| **推論特化チップの活用** | Rebellionsなど専用チップは推論コスト削減に有効 | 車載AI、エッジ推論 |

---

## 📅 関連セッション（同カンファレンス内）

| セッション名 | 日時 | 関連度 | 備考 |
|-------------|------|--------|------|
| Keynote: Ultra Ethernet for AI and HPC | 1/27 09:30-10:15 | ⭐⭐⭐⭐⭐ | ネットワーク技術の詳細 |
| Ethernet for HPC/AI clusters | 1/27 13:30-17:00 | ⭐⭐⭐⭐⭐ | Broadcom主催、Ethernet深掘り |
| R-CCS International Symposium | 1/28 11:00-17:50 | ⭐⭐⭐⭐ | 富岳NEXT構想 |
| Vision and Strategy | 1/27-28 | ⭐⭐⭐⭐ | HPCセンターの将来戦略 |
| Paper Track 8: Accelerators in Practice | 1/28 11:00-12:30 | ⭐⭐⭐⭐ | Cerebras、AMD MI300A等の実践報告 |
| Paper Track 9: Platforms, Clouds & Memory | 1/28 11:00-12:30 | ⭐⭐⭐ | Armプロセッサ性能分析 |

---

## 📚 参考資料

### 📖 論文・プレプリント

- [Hardware Accelerators for Artificial Intelligence (arXiv)](https://arxiv.org/pdf/2411.13717)
- [Scaling the Memory Wall: The Rise and Roadmap of HBM (Semi Analysis)](https://newsletter.semianalysis.com/p/scaling-the-memory-wall-the-rise-and-roadmap-of-hbm)

### 🌐 Webリソース

- [NVIDIA Blackwell Architecture](https://www.nvidia.com/en-us/data-center/technologies/blackwell-architecture/)
- [Arm Neoverse in AI Data Centers](https://newsroom.arm.com/blog/arm-neoverse-in-ai-data-centers)
- [RIKEN R-CCS Leadership](https://www.r-ccs.riken.jp/en/about/leadership/)
- [Rebellions Official Site](https://rebellions.ai/)
- [Google Cloud HPC & AI for Science](https://www.hpcwire.com/2025/11/04/google-cloud-describes-how-to-unite-hpc-and-ai-for-scientific-discovery/)

### 💻 GitHub / 技術ドキュメント

- [Eric Van Hensbergen's GitHub](https://github.com/ericvh)
- [NVIDIA Developer Blog](https://developer.nvidia.com/blog/)
- [Arm Developer Resources](https://developer.arm.com/)

---

## 📖 用語集（高校生向け解説付き）

| 用語 | 読み方 | 説明 |
|------|--------|------|
| **GPU** | ジーピーユー | Graphics Processing Unit。もともとゲームの映像を描くためのチップだが、AIの計算にも最適。数千個の小さな計算ユニットが並列で動くので、「大量の単純作業を同時にこなすアルバイト軍団」のようなもの。 |
| **HBM** | エイチビーエム | High Bandwidth Memory。メモリチップを何層にも積み重ねて超高速化した特殊メモリ。普通のメモリが「1車線道路」なら、HBMは「20車線高速道路」。 |
| **NVLink** | エヌブイリンク | NVIDIAが開発したGPU間を超高速で接続する技術。1秒間に1.8テラバイト（本1億冊分）のデータをやり取りできる。 |
| **InfiniBand** | インフィニバンド | サーバ間を超高速・低遅延で接続するネットワーク技術。スーパーコンピュータの標準。「信号待ちのない高速道路」のような通信。 |
| **Ethernet** | イーサネット | 最も一般的なネットワーク技術。家庭や会社のLANでも使われる。AI向けに「Ultra Ethernet」として進化中。 |
| **液体冷却** | えきたいれいきゃく | 水や専用液体を使ってチップを直接冷やす技術。最新AIチップは1枚で1,400W（電気ストーブ並み）の熱を出すため、空気だけでは冷やせない。 |
| **TDP** | ティーディーピー | Thermal Design Power。チップが発生する熱量の設計上限値。ワット(W)で表す。家庭用ゲームPCのGPUが300W程度、NVIDIA B300は1,400W。 |
| **Arm** | アーム | イギリス発のCPU設計企業。スマホのほぼ全てがArm設計のチップを使用。省電力が特徴で、最近はデータセンターにも進出。 |
| **x86** | エックスはちろく | IntelとAMDが使うCPU設計方式。パソコンやサーバの主流だったが、Armに押されつつある。 |
| **Neoverse** | ネオバース | Armが開発したデータセンター向けCPUプラットフォーム。AWSのGraviton、GoogleのAxionなどがNeoverse採用。 |
| **エクサスケール** | エクサスケール | 1秒間に100京回（10^18回）の計算ができる性能。富岳がほぼこの水準。「エクサ」は10^18を表す接頭辞。 |
| **ゼタスケール** | ゼタスケール | エクサの1,000倍。1秒間に10垓回（10^21回）の計算。富岳NEXTが目指す水準。 |
| **テンソルコア** | テンソルコア | NVIDIAのGPUに搭載された、AI計算専用の回路。行列演算（AIの基本計算）を超高速で行う。 |
| **FLOPS** | フロップス | Floating Point Operations Per Second。1秒間に何回の小数計算ができるかを表す単位。PFLOPS（ペタ）= 10^15回/秒、EFLOPS（エクサ）= 10^18回/秒。 |
| **推論（Inference）** | すいろん | 学習済みAIモデルを使って予測・判断を行うこと。ChatGPTの回答生成も推論。学習より計算量は少ないが、リアルタイム性が求められる。 |
| **学習（Training）** | がくしゅう | AIモデルにデータを読み込ませて賢くすること。GPT-4の学習には数十億ドル規模の計算資源が必要だった。 |
| **スケールアップ** | スケールアップ | 1台のマシンを高性能化すること。GPUを増やす、メモリを増やすなど。 |
| **スケールアウト** | スケールアウト | マシンの台数を増やして全体性能を上げること。ネットワークで多数のサーバを接続。 |
| **ハイパースケーラー** | ハイパースケーラー | AWS、Google、Microsoft、Metaなど、巨大なデータセンターを運営する企業。数十万台のサーバを保有。 |
| **AIファクトリー** | エーアイファクトリー | NVIDIAが提唱する概念。AIモデルを「製造」する専用施設。従来のデータセンターとは異なる設計思想。 |
| **Co-packaged Optics** | コパッケージドオプティクス | CPO。光通信モジュールをチップと一体化する技術。電気信号を光に変換する距離を短くし、消費電力と遅延を削減。 |
| **PUE** | ピーユーイー | Power Usage Effectiveness。データセンターの電力効率指標。「全消費電力÷IT機器消費電力」で計算。1.0に近いほど効率が良い。最新の液冷DCは1.2以下。 |
| **チップレット** | チップレット | 複数の小さなチップを組み合わせて1つの大きなチップのように動かす技術。製造歩留まりの向上と柔軟な設計が可能。 |

---

*作成日: 2025-12-25*
*SCA/HPC Asia 2026 事前勉強資料*
